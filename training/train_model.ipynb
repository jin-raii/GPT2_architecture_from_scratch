{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf011af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "from pathlib import Path \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed384e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = Path(os.getcwd()).resolve().parent \n",
    "sys.path.insert(0, str(parent_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97f85ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jin/gpt2_from_scratch')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5766732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jin/gpt2_from_scratch/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent_folder  /home/jin/gpt2_from_scratch/gpt2\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from gpt2_config.config import GPT2Config\n",
    "from gpt2.gpt_model import GPT2\n",
    "from data.dataset import InstructDataset, collate_fn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "539bf806",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('../final_dataset')\n",
    "config = GPT2Config()\n",
    "model = GPT2(cfg=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf92b1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0020c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "train_ids = InstructDataset(dataset['train'], tokenizer)\n",
    "val_ids = InstructDataset(dataset['val'], tokenizer)\n",
    "test_ids = InstructDataset(dataset['test'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aed148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ids, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn, \n",
    "    drop_last=True, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ids, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn, \n",
    "    drop_last=True, \n",
    "    num_workers=0 \n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_ids, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=collate_fn, \n",
    "    shuffle=True, \n",
    "    drop_last=True, \n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d2c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed4ed3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device =  'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35128571",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.to(device).parameters(), lr=5e-5, weight_decay=0.1)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "674a79eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    decoupled_weight_decay: True\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 5e-05\n",
       "    maximize: False\n",
       "    weight_decay: 0.1\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14011c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc121fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 (Step 000000): Train loss 10.327, Vall loss 10.252\n",
      "epoch 1 (Step 000003): Train loss 8.906, Vall loss 9.120\n",
      "epoch 1 (Step 000006): Train loss 7.827, Vall loss 8.856\n",
      "epoch 1 (Step 000009): Train loss 8.831, Vall loss 8.854\n",
      "epoch 1 (Step 000012): Train loss 8.276, Vall loss 8.190\n",
      "epoch 1 (Step 000015): Train loss 7.537, Vall loss 7.873\n",
      "epoch 1 (Step 000018): Train loss 7.265, Vall loss 7.263\n",
      "epoch 1 (Step 000021): Train loss 7.459, Vall loss 7.051\n",
      "epoch 1 (Step 000024): Train loss 6.520, Vall loss 6.256\n",
      "epoch 1 (Step 000027): Train loss 6.226, Vall loss 6.629\n",
      "epoch 1 (Step 000030): Train loss 6.711, Vall loss 5.928\n",
      "epoch 1 (Step 000033): Train loss 6.580, Vall loss 6.681\n",
      "epoch 1 (Step 000036): Train loss 6.468, Vall loss 6.563\n",
      "epoch 1 (Step 000039): Train loss 5.298, Vall loss 5.615\n",
      "epoch 1 (Step 000042): Train loss 6.467, Vall loss 6.568\n",
      "epoch 1 (Step 000045): Train loss 5.446, Vall loss 5.922\n",
      "epoch 1 (Step 000048): Train loss 6.147, Vall loss 5.210\n",
      "epoch 1 (Step 000051): Train loss 5.492, Vall loss 5.689\n",
      "epoch 1 (Step 000054): Train loss 5.320, Vall loss 5.612\n",
      "epoch 1 (Step 000057): Train loss 5.211, Vall loss 5.608\n",
      "epoch 1 (Step 000060): Train loss 5.217, Vall loss 5.286\n",
      "epoch 1 (Step 000063): Train loss 4.760, Vall loss 4.836\n",
      "epoch 1 (Step 000066): Train loss 5.210, Vall loss 4.844\n",
      "epoch 1 (Step 000069): Train loss 4.820, Vall loss 4.981\n",
      "epoch 1 (Step 000072): Train loss 5.209, Vall loss 5.251\n",
      "epoch 1 (Step 000075): Train loss 4.326, Vall loss 5.384\n",
      "epoch 1 (Step 000078): Train loss 5.756, Vall loss 5.101\n",
      "epoch 1 (Step 000081): Train loss 5.186, Vall loss 5.007\n",
      "epoch 1 (Step 000084): Train loss 5.066, Vall loss 5.286\n",
      "epoch 1 (Step 000087): Train loss 4.641, Vall loss 5.077\n",
      "epoch 1 (Step 000090): Train loss 4.271, Vall loss 5.535\n",
      "epoch 1 (Step 000093): Train loss 4.671, Vall loss 5.355\n",
      "epoch 1 (Step 000096): Train loss 5.075, Vall loss 5.040\n",
      "epoch 1 (Step 000099): Train loss 5.457, Vall loss 4.712\n",
      "epoch 1 (Step 000102): Train loss 3.659, Vall loss 5.227\n",
      "epoch 1 (Step 000105): Train loss 4.793, Vall loss 4.495\n",
      "epoch 1 (Step 000108): Train loss 4.449, Vall loss 4.141\n",
      "epoch 1 (Step 000111): Train loss 3.945, Vall loss 5.030\n",
      "epoch 1 (Step 000114): Train loss 5.236, Vall loss 4.967\n",
      "epoch 1 (Step 000117): Train loss 5.028, Vall loss 4.161\n",
      "epoch 1 (Step 000120): Train loss 4.231, Vall loss 3.995\n",
      "epoch 1 (Step 000123): Train loss 4.255, Vall loss 3.974\n",
      "epoch 1 (Step 000126): Train loss 4.947, Vall loss 4.529\n",
      "epoch 1 (Step 000129): Train loss 4.700, Vall loss 4.608\n",
      "epoch 1 (Step 000132): Train loss 4.527, Vall loss 5.577\n",
      "epoch 1 (Step 000135): Train loss 4.767, Vall loss 4.418\n",
      "epoch 1 (Step 000138): Train loss 4.840, Vall loss 5.313\n",
      "epoch 1 (Step 000141): Train loss 4.092, Vall loss 4.286\n",
      "epoch 1 (Step 000144): Train loss 4.377, Vall loss 4.199\n",
      "epoch 1 (Step 000147): Train loss 4.431, Vall loss 4.486\n",
      "epoch 1 (Step 000150): Train loss 4.214, Vall loss 4.530\n",
      "epoch 1 (Step 000153): Train loss 4.618, Vall loss 4.621\n",
      "epoch 1 (Step 000156): Train loss 3.874, Vall loss 4.521\n",
      "epoch 1 (Step 000159): Train loss 4.443, Vall loss 4.474\n",
      "epoch 1 (Step 000162): Train loss 4.389, Vall loss 4.450\n",
      "epoch 1 (Step 000165): Train loss 3.968, Vall loss 4.885\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, optimizer, device, num_epochs=1, eval_freq=3, eval_iter=2, start_context=4, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d1ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
